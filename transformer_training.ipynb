{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8166783,"sourceType":"datasetVersion","datasetId":4832688},{"sourceId":8199141,"sourceType":"datasetVersion","datasetId":4857001},{"sourceId":8235315,"sourceType":"datasetVersion","datasetId":4884475}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive/')","metadata":{"id":"YlSIi7Cw9AEJ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b204947d-dd3e-4f0f-9758-3b6815051d26"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"}]},{"cell_type":"code","source":"import torch\nimport torchvision.transforms as transforms\nfrom torchvision.models import vit_b_16, ViT_B_16_Weights\nfrom timm.models import create_model\n\n\nfrom torch.utils.data import DataLoader, random_split, Dataset\nfrom torchvision.datasets import ImageFolder\nimport torch.nn as nn\nimport torch.optim as optim\nimport pandas as pd\nimport os\nfrom torchvision.io import read_image\nfrom torchvision.transforms import Compose, Resize, ToTensor\nfrom PIL import Image","metadata":{"id":"_hYl_5LHnTf4","execution":{"iopub.status.busy":"2024-05-14T10:44:19.205811Z","iopub.execute_input":"2024-05-14T10:44:19.206088Z","iopub.status.idle":"2024-05-14T10:44:26.225358Z","shell.execute_reply.started":"2024-05-14T10:44:19.206064Z","shell.execute_reply":"2024-05-14T10:44:26.224594Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"\ntrain_dir = \"/kaggle/input//trees-dataset/vkr\"\nvalid_dir = \"/kaggle/input//trees-dataset/vkr\"\ntest = \"test\"\ntrain_data=  pd.read_csv('/kaggle/input/trees-dataset/vkr/new_train.csv')\nvalid_data = pd.read_csv('/kaggle/input//trees-dataset/vkr/new_valid.csv')\n\n\ntrain_data.class_num = train_data.class_num.astype(str)\nvalid_data.class_num = valid_data.class_num.astype(str)","metadata":{"id":"f4c4dff8","execution":{"iopub.status.busy":"2024-05-14T10:44:33.246963Z","iopub.execute_input":"2024-05-14T10:44:33.247516Z","iopub.status.idle":"2024-05-14T10:44:33.282065Z","shell.execute_reply.started":"2024-05-14T10:44:33.247468Z","shell.execute_reply":"2024-05-14T10:44:33.281101Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def check_files_existence(df, directory, column):\n    invalid_files = []\n    for filepath in df[column]:\n        if not os.path.exists(os.path.join(directory, filepath)):\n            invalid_files.append(filepath)\n    return invalid_files","metadata":{"id":"60a59f50","execution":{"iopub.status.busy":"2024-05-14T10:44:48.464392Z","iopub.execute_input":"2024-05-14T10:44:48.464986Z","iopub.status.idle":"2024-05-14T10:44:48.470075Z","shell.execute_reply.started":"2024-05-14T10:44:48.464957Z","shell.execute_reply":"2024-05-14T10:44:48.469080Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"invalid_train_files = check_files_existence(train_data, train_dir, 'id')\ninvalid_valid_files = check_files_existence(valid_data, valid_dir, 'id')","metadata":{"id":"043e0888","execution":{"iopub.status.busy":"2024-05-14T10:45:00.719672Z","iopub.execute_input":"2024-05-14T10:45:00.720563Z","iopub.status.idle":"2024-05-14T10:45:02.139818Z","shell.execute_reply.started":"2024-05-14T10:45:00.720526Z","shell.execute_reply":"2024-05-14T10:45:02.138617Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def replace_extension_train(row):\n    if row['id'] in invalid_train_files:\n        return row['id'].replace('.jpg', '.JPG')\n    else:\n        return row['id']\n\ndef replace_extension_valid(row):\n    if row['id'] in invalid_valid_files:\n        return row['id'].replace('.jpg', '.JPG')\n    else:\n        return row['id']\n\n# Применение функции к DataFrame\ntrain_data['id'] = train_data.apply(replace_extension_train, axis=1)\nvalid_data['id'] = valid_data.apply(replace_extension_valid, axis=1)","metadata":{"id":"6R2sXnAT6X76","execution":{"iopub.status.busy":"2024-05-14T10:44:57.712847Z","iopub.execute_input":"2024-05-14T10:44:57.713717Z","iopub.status.idle":"2024-05-14T10:44:57.767018Z","shell.execute_reply.started":"2024-05-14T10:44:57.713684Z","shell.execute_reply":"2024-05-14T10:44:57.766300Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"print(\"Недействительные файлы в обучающем наборе:\", invalid_train_files)\nprint(\"Недействительные файлы в валидационном наборе:\", invalid_valid_files)","metadata":{"id":"3166a36d","outputId":"4d192522-b82c-4838-ccd8-d42c9eea9b48","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2024-05-14T10:45:05.096052Z","iopub.execute_input":"2024-05-14T10:45:05.096664Z","iopub.status.idle":"2024-05-14T10:45:05.101848Z","shell.execute_reply.started":"2024-05-14T10:45:05.096635Z","shell.execute_reply":"2024-05-14T10:45:05.100813Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Недействительные файлы в обучающем наборе: []\nНедействительные файлы в валидационном наборе: []\n","output_type":"stream"}]},{"cell_type":"code","source":"train_data_df = train_data.copy()\nvalid_data_df = valid_data.copy()","metadata":{"id":"hihwzlxk-xzC","execution":{"iopub.status.busy":"2024-05-14T10:45:08.060753Z","iopub.execute_input":"2024-05-14T10:45:08.061623Z","iopub.status.idle":"2024-05-14T10:45:08.068557Z","shell.execute_reply.started":"2024-05-14T10:45:08.061591Z","shell.execute_reply":"2024-05-14T10:45:08.067830Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_img_dir = '/kaggle/input/trees-dataset/vkr/'\nval_img_dir = '/kaggle/input/trees-dataset/vkr/'\n\n\ndef read_image_pil(img_path):\n    with Image.open(img_path).convert('RGB') as img:\n        return ToTensor()(img)\n\nclass TreesDataset(Dataset):\n    def __init__(self, annotations_file, img_dir, transform=None):\n        self.img_labels = annotations_file\n        self.transform = transform\n        self.img_dir = img_dir\n\n    def __len__(self):\n        return len(self.img_labels)\n\n    def __getitem__(self, idx):\n        img_path = self.img_labels.iloc[idx, 0]\n        image = read_image_pil(self.img_dir + img_path)  \n        label = self.img_labels.iloc[idx, 1]\n        if self.transform:\n            image = self.transform(image)\n        label = int(label)\n        return image, torch.tensor(label)\n\n# transform = transforms.Compose([\n#     transforms.RandomResizedCrop(224),\n#     transforms.RandomHorizontalFlip(),\n#     # transforms.ToTensor(),\n#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n# ])\n\n# transform = transforms.Compose([\n#     transforms.RandomResizedCrop(224),\n#     transforms.RandomHorizontalFlip(),\n#     transforms.RandomRotation(15),\n# ])\n# transform = transforms.Compose([\n#     transforms.RandomResizedCrop(224),\n#     transforms.RandomHorizontalFlip(),\n#     transforms.RandomRotation(15),\n#     transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1),\n#     transforms.RandomGrayscale(p=0.2),\n#     transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n# ])\n\n\ntransform = transforms.Compose([\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.RandomGrayscale(p=0.2),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# transform = transforms.Compose([\n#     transforms.RandomResizedCrop(224),\n#     transforms.RandomHorizontalFlip(),\n#     transforms.RandomRotation(15),\n#     transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1),\n#     # transforms.ToTensor(),\n#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n# ])\n\ntrain_data = TreesDataset(train_data_df, train_img_dir, transform=transform)\nvalid_data = TreesDataset(valid_data_df, val_img_dir, transform=transform)\n\ntrain_loader = DataLoader(train_data, batch_size=32)\nvalid_loader = DataLoader(valid_data, batch_size=32)\n\nmodel = create_model(\n    'deit_base_patch16_224',\n    pretrained=True,\n    num_classes=39\n)\n\n# model = vit_b_16(pretrained=True)\n# for param in model.parameters():\n#     param.requires_grad = False  \n\nfor param in model.parameters():\n    param.requires_grad = False \n\n\n# for param in model.head.parameters():\n#     param.requires_grad = True\n\nfor param in model.blocks[-1].parameters():  \n    param.requires_grad = True\n\n\n# optimizer = torch.optim.AdamW([\n#     {'params': model.head.parameters(), 'lr': 1e-3},\n#     {'params': model.blocks[-1].parameters(), 'lr': 1e-4}\n# ], lr=1e-4, weight_decay=0.01)\n# for param in model.parameters():\n#     param.requires_grad = True\n\n# for layer in model.blocks[:-3]:  \n#     for param in layer.parameters():\n#         param.requires_grad = False\n\n\n# original_head_layer = model.heads.head\n# model.heads = nn.Sequential(nn.Linear(in_features=original_head_layer.in_features,\n#                                       out_features=39,\n#                                       bias=True))\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW([\n    {'params': model.head.parameters(), 'lr': 0.001},\n    {'params': model.blocks.parameters(), 'lr': 0.0001}\n], lr=0.00001, weight_decay=0.01)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-14T10:45:45.892982Z","iopub.execute_input":"2024-05-14T10:45:45.893406Z","iopub.status.idle":"2024-05-14T10:45:49.577536Z","shell.execute_reply.started":"2024-05-14T10:45:45.893375Z","shell.execute_reply":"2024-05-14T10:45:49.576728Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccd64bfe77bf427ab14574169433da54"}},"metadata":{}}]},{"cell_type":"code","source":"def train(model, criterion, optimizer, train_loader, valid_loader, epochs=25):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    if torch.cuda.is_available():\n        print(\"gpu\")\n    else:\n        print(\"cpu\")\n    model.to(device)\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0\n        total = 0\n        correct = 0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n\n        model.eval()\n        valid_loss = 0\n        with torch.no_grad():\n            for images, labels in valid_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                valid_loss += loss.item()\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n        print(f\"Validation Loss: {valid_loss/len(valid_loader)}\")\n        print(f'Accuracy of the network on the validation images: {100 * correct / total}%')\n\ntrain(model, criterion, optimizer, train_loader, valid_loader)","metadata":{"scrolled":true,"id":"cb3a6a9b","outputId":"e1e538bd-6329-4dc1-bc17-4dcd64c684b5","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2024-04-22T22:35:27.715674Z","iopub.execute_input":"2024-04-22T22:35:27.716141Z","iopub.status.idle":"2024-04-22T23:15:51.281859Z","shell.execute_reply.started":"2024-04-22T22:35:27.716105Z","shell.execute_reply":"2024-04-22T23:15:51.280774Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0786da7a43f14de6bf04b562fc606ebb"}},"metadata":{}},{"name":"stdout","text":"gpu\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 3.7360418664998023\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/PIL/Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 3.359498014816871\nAccuracy of the network on the validation images: 16.160388821385176%\nEpoch 2, Loss: 3.3249988350374946\nValidation Loss: 2.987804862169119\nAccuracy of the network on the validation images: 21.992709599027947%\nEpoch 3, Loss: 2.972170729061653\nValidation Loss: 2.7137078092648435\nAccuracy of the network on the validation images: 27.095990279465372%\nEpoch 4, Loss: 2.6996807908189706\nValidation Loss: 2.54224621791106\nAccuracy of the network on the validation images: 31.591737545565007%\nEpoch 5, Loss: 2.474110644439171\nValidation Loss: 2.333836243702815\nAccuracy of the network on the validation images: 35.47995139732685%\nEpoch 6, Loss: 2.2900569500594306\nValidation Loss: 2.2216145396232605\nAccuracy of the network on the validation images: 39.611178614823814%\nEpoch 7, Loss: 2.14720317413067\nValidation Loss: 2.0738272070884705\nAccuracy of the network on the validation images: 43.377885783718106%\nEpoch 8, Loss: 1.966378643594939\nValidation Loss: 2.0502251340792728\nAccuracy of the network on the validation images: 42.64884568651276%\nEpoch 9, Loss: 1.8934317806671406\nValidation Loss: 1.9689868940756872\nAccuracy of the network on the validation images: 44.349939246658565%\nEpoch 10, Loss: 1.7707582188063655\nValidation Loss: 1.8852329620948205\nAccuracy of the network on the validation images: 48.11664641555286%\nEpoch 11, Loss: 1.6646938015674722\nValidation Loss: 1.8684254999344165\nAccuracy of the network on the validation images: 48.11664641555286%\nEpoch 12, Loss: 1.5552043205705182\nValidation Loss: 1.7974473948662097\nAccuracy of the network on the validation images: 50.182260024301335%\nEpoch 13, Loss: 1.5176648086514966\nValidation Loss: 1.727378643476046\nAccuracy of the network on the validation images: 52.369380315917375%\nEpoch 14, Loss: 1.4244055378025975\nValidation Loss: 1.676113371665661\nAccuracy of the network on the validation images: 52.12636695018226%\nEpoch 15, Loss: 1.3715631345222736\nValidation Loss: 1.6682519912719727\nAccuracy of the network on the validation images: 52.97691373025516%\nEpoch 16, Loss: 1.2778767172632546\nValidation Loss: 1.600039074054131\nAccuracy of the network on the validation images: 51.88335358444714%\nEpoch 17, Loss: 1.2596773238017642\nValidation Loss: 1.5828655339204347\nAccuracy of the network on the validation images: 54.07047387606318%\nEpoch 18, Loss: 1.1994370550944888\nValidation Loss: 1.602333048215279\nAccuracy of the network on the validation images: 52.97691373025516%\nEpoch 19, Loss: 1.143821459648938\nValidation Loss: 1.5734449166517992\nAccuracy of the network on the validation images: 54.556500607533415%\nEpoch 20, Loss: 1.0854648161551048\nValidation Loss: 1.5519301707927997\nAccuracy of the network on the validation images: 54.434993924665854%\nEpoch 21, Loss: 1.0204258848880898\nValidation Loss: 1.5788883819029882\nAccuracy of the network on the validation images: 53.827460510328066%\nEpoch 22, Loss: 0.9788597277526198\nValidation Loss: 1.4953955457760737\nAccuracy of the network on the validation images: 56.13608748481167%\nEpoch 23, Loss: 0.9432091206826013\nValidation Loss: 1.5386407925532415\nAccuracy of the network on the validation images: 55.1640340218712%\nEpoch 24, Loss: 0.9260787650428969\nValidation Loss: 1.501241175027994\nAccuracy of the network on the validation images: 56.86512758201701%\nEpoch 25, Loss: 0.8733849304503408\nValidation Loss: 1.5125477474469404\nAccuracy of the network on the validation images: 56.622114216281894%\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'model_weights.pth')","metadata":{"id":"aSTyMk3rrhVM","execution":{"iopub.status.busy":"2024-04-22T23:22:09.507184Z","iopub.execute_input":"2024-04-22T23:22:09.507602Z","iopub.status.idle":"2024-04-22T23:22:10.955989Z","shell.execute_reply.started":"2024-04-22T23:22:09.507570Z","shell.execute_reply":"2024-04-22T23:22:10.954881Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def process_image(image_path):\n    image = Image.open(image_path)\n\n    transformation = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n    ])\n\n    processed_image = transformation(image).unsqueeze(0).to(device)\n\n    return processed_image","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:32:17.273148Z","iopub.execute_input":"2024-04-22T23:32:17.273561Z","iopub.status.idle":"2024-04-22T23:32:17.280414Z","shell.execute_reply.started":"2024-04-22T23:32:17.273529Z","shell.execute_reply":"2024-04-22T23:32:17.279351Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Используется устройство:\", device)\n\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:30:55.795065Z","iopub.execute_input":"2024-04-22T23:30:55.795690Z","iopub.status.idle":"2024-04-22T23:30:55.809978Z","shell.execute_reply.started":"2024-04-22T23:30:55.795661Z","shell.execute_reply":"2024-04-22T23:30:55.808869Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Используется устройство: cuda\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"VisionTransformer(\n  (patch_embed): PatchEmbed(\n    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n    (norm): Identity()\n  )\n  (pos_drop): Dropout(p=0.0, inplace=False)\n  (patch_drop): Identity()\n  (norm_pre): Identity()\n  (blocks): Sequential(\n    (0): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (1): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (2): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (3): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (4): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (5): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (6): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (7): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (8): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (9): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (10): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (11): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n  )\n  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n  (fc_norm): Identity()\n  (head_drop): Dropout(p=0.0, inplace=False)\n  (head): Linear(in_features=768, out_features=39, bias=True)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"model","metadata":{}},{"cell_type":"code","source":"model = torch.load('/kaggle/input/models-trees/full_model.pth')\nprint(\"Модель успешно загружена.\")\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:23:48.446240Z","iopub.execute_input":"2024-04-22T23:23:48.446897Z","iopub.status.idle":"2024-04-22T23:23:48.697775Z","shell.execute_reply.started":"2024-04-22T23:23:48.446868Z","shell.execute_reply":"2024-04-22T23:23:48.696755Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Модель успешно загружена.\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"VisionTransformer(\n  (patch_embed): PatchEmbed(\n    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n    (norm): Identity()\n  )\n  (pos_drop): Dropout(p=0.0, inplace=False)\n  (patch_drop): Identity()\n  (norm_pre): Identity()\n  (blocks): Sequential(\n    (0): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (1): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (2): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (3): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (4): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (5): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (6): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (7): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (8): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (9): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (10): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (11): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n  )\n  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n  (fc_norm): Identity()\n  (head_drop): Dropout(p=0.0, inplace=False)\n  (head): Linear(in_features=768, out_features=39, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"import torch.nn.functional as F\ndef predict(image_path):\n    image = process_image(image_path) \n    output = model(image)\n    probabilities = F.softmax(output, dim=1) \n    probabilities = probabilities.squeeze().tolist()\n\n    class_probabilities = {i: prob for i, prob in enumerate(probabilities)}\n\n    return class_probabilities\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:38:53.171752Z","iopub.execute_input":"2024-04-22T23:38:53.172688Z","iopub.status.idle":"2024-04-22T23:38:53.179067Z","shell.execute_reply.started":"2024-04-22T23:38:53.172657Z","shell.execute_reply":"2024-04-22T23:38:53.178019Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"predict('/kaggle/input/klentree/.jpg')","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:38:53.990395Z","iopub.execute_input":"2024-04-22T23:38:53.990766Z","iopub.status.idle":"2024-04-22T23:38:54.042953Z","shell.execute_reply.started":"2024-04-22T23:38:53.990740Z","shell.execute_reply":"2024-04-22T23:38:54.041994Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"{0: 0.19968588650226593,\n 1: 0.0007737760315649211,\n 2: 0.07776502519845963,\n 3: 0.0009698724024929106,\n 4: 0.002414742251858115,\n 5: 0.002116408199071884,\n 6: 0.0028157804626971483,\n 7: 0.0010858316672965884,\n 8: 0.0015513594262301922,\n 9: 0.0016245472943410277,\n 10: 0.000620123406406492,\n 11: 0.000667078304104507,\n 12: 0.003822344122454524,\n 13: 0.001060699112713337,\n 14: 0.627441942691803,\n 15: 0.000508010620251298,\n 16: 0.0005058939568698406,\n 17: 0.0014427549904212356,\n 18: 0.004695465322583914,\n 19: 0.0010829318780452013,\n 20: 0.020313667133450508,\n 21: 0.0022266958840191364,\n 22: 0.002123433630913496,\n 23: 0.003813183633610606,\n 24: 0.0018579134484753013,\n 25: 0.0016312720254063606,\n 26: 0.0009638582123443484,\n 27: 0.0033224120270460844,\n 28: 0.001822005258873105,\n 29: 0.0033516166731715202,\n 30: 0.010313984006643295,\n 31: 0.0015764555428177118,\n 32: 0.008202550932765007,\n 33: 0.0003408501506783068,\n 34: 0.0006209814455360174,\n 35: 0.0008583302260376513,\n 36: 0.0012904965551570058,\n 37: 0.0005025874124839902,\n 38: 0.0022172818426042795}"},"metadata":{}}]},{"cell_type":"code","source":"import torch.nn.functional as F\nimport torch\nimport torchvision.transforms as transforms\n\n\nclass Predictor:\n    def __init__(self, model_path):\n        self.model_path = model_path\n        \n        self.model = torch.load(self.model_path)\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model.to(device)\n        self.model.eval()\n        \n    def process_image(self, image_path):\n        image = Image.open(image_path)\n\n        transformation = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n        ])\n\n        processed_image = transformation(image).unsqueeze(0).to(device)\n\n        return processed_image\n    \n    \n    def predict(self, image_path):\n        image = process_image(self, image_path) \n        output = self.model(image)\n        probabilities = F.softmax(output, dim=1) \n        probabilities = probabilities.squeeze().tolist()\n\n        class_probabilities = {i: prob for i, prob in enumerate(probabilities)}\n\n        return class_probabilities","metadata":{"execution":{"iopub.status.busy":"2024-04-26T09:46:15.159931Z","iopub.execute_input":"2024-04-26T09:46:15.160612Z","iopub.status.idle":"2024-04-26T09:46:15.171258Z","shell.execute_reply.started":"2024-04-26T09:46:15.160577Z","shell.execute_reply":"2024-04-26T09:46:15.170183Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"predictor = Predictor('/kaggle/input/models-trees/full_model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-04-26T09:46:16.142007Z","iopub.execute_input":"2024-04-26T09:46:16.142675Z","iopub.status.idle":"2024-04-26T09:46:16.423407Z","shell.execute_reply.started":"2024-04-26T09:46:16.142643Z","shell.execute_reply":"2024-04-26T09:46:16.422341Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"predictor.predict('/kaggle/input/klentree/.jpg')","metadata":{"execution":{"iopub.status.busy":"2024-04-26T09:46:16.425082Z","iopub.execute_input":"2024-04-26T09:46:16.425396Z","iopub.status.idle":"2024-04-26T09:46:16.482232Z","shell.execute_reply.started":"2024-04-26T09:46:16.425351Z","shell.execute_reply":"2024-04-26T09:46:16.480928Z"},"trusted":true},"execution_count":25,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/klentree/.jpg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[23], line 30\u001b[0m, in \u001b[0;36mPredictor.predict\u001b[0;34m(self, image_path)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, image_path):\n\u001b[0;32m---> 30\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_image\u001b[49m(\u001b[38;5;28mself\u001b[39m, image_path) \n\u001b[1;32m     31\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(image)\n\u001b[1;32m     32\u001b[0m     probabilities \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \n","\u001b[0;31mNameError\u001b[0m: name 'process_image' is not defined"],"ename":"NameError","evalue":"name 'process_image' is not defined","output_type":"error"}]}]}